### 2021.08.10 Tue  

#
- week2 모더레이터 : 손희락
- 참가자 : 박별이, 이상준, 이은우, 장재현, 전상민, 최성원
#
1. AI Boost camp
- DL Basic 3강, Data Viz 2-1 ~ 2-3강
- DL Basic 3강 Gradient Descent Method 논의

  
2. 알고리즘 스터디
- 백준 [7562](https://www.acmicpc.net/problem/7562), [10026](https://www.acmicpc.net/problem/10026) 문제 코드 리뷰  


  
3. Q & A
- DFS 문제 구현에서 stack과 재귀사용법. BFS와 비교

- Q : Adagrad의 방법에서 Gt는 어떻게 구할 수 있을까? 
  A : 각각의 parameter들의 합을 벡터나 행렬의 꼴로 저장?
  
- Q : Adagrad의 방법에서 G에 현재 갱신된 가중치 gt의 값의 제곱도 더해져 계산될 까? 
  * A : 밑의 Adadelta의 식을 보아 현재 gt의 값도 더해져 계산되는 것 같다.

- Q : Adagrad의 방법에서 gt 값에 관계없이 1보다 작은 값이 곱해지니까 과정이 진행될수록 기존의 gt보다는 작아질 것 같은데, 작게 변화한 파라미터는 빠르게 변화하도록 한다는 의미는 어떻게 이해하면 될까? 
  * A : 빠르게 변화한다는 것이 각각의 가중치끼리 비교한 상대적인 빠르기가 아닐까? 혹은, 같은 과정에서 본다면 값이 크게 변화했을 때 보다, 적게 변했을 때 빠르다는 의미 인가? 
  
- Q : Adadelta의 방법에서 윈도우 사이즈가 파라미터의 수가 많아짐에 따라 탐색하는 것이 왜 불가능해질까?
  * A : 윈도우의 각각의 요소에 대해 파라미터가 적용되어 파라미터가 많아진다면, 탐색이 힘들 것 같다.
  
- Q : Adadelta에서 exponentially moving average가 어떻게 윈도우 사이즈의 역할을 할 수 있을까? (윈도우 사이즈는 단순합이고 exp ma는 각 시점에서 가중치가 주어져서, 오래된 시점에 대해서는 조금의 정보만 얻는 것인데)
  * A : 윈도우 사이즈는 오래된 시점을 제외하고 계산하게 되는데, 이것이 exp ma에서 오래된 시점일수록 값이 매우 작아져 무시하는 효과와 같은 것이 아닐까? 

- Q : Adadelta의 방법에서 Ht의 정체는 무엇일까?


  
#
### 2021.08.11 Wed - To Do 
#
1. AI Boost camp
- 새로운 강의 듣고 어려운 내용 질문 & 답변
- 과제 풀어보고 간단한 아이디어 나누기
- 질문 게시판의 다른 캠퍼분들의 질문들 생각해보기  


  
2. 알고리즘 스터디
- 백준 [5710](https://www.acmicpc.net/problem/5710) 문제 풀고 코드 리뷰

